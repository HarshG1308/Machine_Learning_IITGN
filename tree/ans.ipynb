{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from graphviz import Digraph  # Add this import for Digraph\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "@dataclass\n",
    "class DecisionTree:\n",
    "    criterion: Literal[\"information_gain\", \"gini_index\"]\n",
    "    max_depth: int\n",
    "\n",
    "    def _init_(self, criterion, max_depth=5):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None  # Add a property to store the constructed tree\n",
    "\n",
    "    def find_best_split(self, X: pd.DataFrame, y: pd.Series) -> Tuple[float, int, float]:\n",
    "        \"\"\"Finds the best split for the current node.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of (split_value, split_feature, split_gain).\n",
    "        \"\"\"\n",
    "        best_split_value = None\n",
    "        best_split_feature = None\n",
    "        best_split_gain = float('-inf')  # or float('inf') depending on the criterion\n",
    "\n",
    "        # Your implementation here, calculate split_value, split_feature, and split_gain\n",
    "\n",
    "        # Example:\n",
    "        # best_split_value = calculated_value\n",
    "        # best_split_feature = calculated_feature\n",
    "        # best_split_gain = calculated_gain\n",
    "\n",
    "        return best_split_value, best_split_feature, best_split_gain\n",
    "\n",
    "    # ... rest of your class methods ...\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        \"\"\"\n",
    "        Function to train and construct the decision tree\n",
    "        \"\"\"\n",
    "        self.tree = self._fit(X, y, depth=0)\n",
    "\n",
    "    def _fit(self, X: pd.DataFrame, y: pd.Series, depth: int) -> dict:\n",
    "        \"\"\"\n",
    "        Recursive function to construct the decision tree\n",
    "        \"\"\"\n",
    "        if depth == self.max_depth or len(set(y)) == 1:\n",
    "            # Stop recursion if max depth reached or all labels are the same\n",
    "            return {'value': y.mode().iloc[0]}\n",
    "\n",
    "        # Find the best split\n",
    "        split_value, split_feature, split_gain = self.find_best_split(X, y)\n",
    "\n",
    "        if split_feature is None:\n",
    "            # If no split is found, create a leaf node\n",
    "            return {'value': y.mode().iloc[0]}\n",
    "\n",
    "        # Split the dataset based on the best split\n",
    "        left_mask = X[split_feature] <= split_value\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        # Recursively construct the left and right subtrees\n",
    "        left_subtree = self._fit(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._fit(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        # Create a decision node\n",
    "        return {'feature': split_feature,\n",
    "                'value': split_value,\n",
    "                'left': left_subtree,\n",
    "                'right': right_subtree}\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Function to run the decision tree on test inputs\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        for _, row in X.iterrows():\n",
    "            predictions.append(self._predict(self.tree, row))\n",
    "\n",
    "        return pd.Series(predictions)\n",
    "\n",
    "    def _predict(self, node: dict, row: pd.Series) -> int:\n",
    "        \"\"\"\n",
    "        Recursive function to make predictions using the decision tree\n",
    "        \"\"\"\n",
    "        if 'value' in node:\n",
    "            # Leaf node, return the predicted value\n",
    "            return node['value']\n",
    "        else:\n",
    "            # Decision node, traverse to the appropriate subtree\n",
    "            if row[node['feature']] <= node['value']:\n",
    "                return self._predict(node['left'], row)\n",
    "            else:\n",
    "                return self._predict(node['right'], row)\n",
    "\n",
    "    def plot(self, node=None, depth=0, parent_name=None, graph=None) -> None:\n",
    "        \"\"\"\n",
    "        Function to plot the tree\n",
    "        \"\"\"\n",
    "        if graph is None:\n",
    "            graph = Digraph(format='png', node_attr={'style': 'filled', 'fillcolor': 'lightblue'})\n",
    "            graph.attr(size='10,10')\n",
    "\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "\n",
    "        if 'value' in node:\n",
    "            graph.node(str(parent_name), label=str(node['value']), shape='ellipse', color='black', fillcolor='yellow')\n",
    "            return\n",
    "\n",
    "        feature_name = node['feature']\n",
    "        value = node['value']\n",
    "\n",
    "        graph.node(str(parent_name), label=f\"{feature_name} <= {value}\", shape='box', color='black', fillcolor='lightblue')\n",
    "\n",
    "        left_name = f\"{parent_name}_L\"\n",
    "        self.plot(node['left'], depth + 1, left_name, graph)\n",
    "        graph.edge(str(parent_name), left_name, label='Yes')\n",
    "\n",
    "        right_name = f\"{parent_name}_R\"\n",
    "        self.plot(node['right'], depth + 1, right_name, graph)\n",
    "        graph.edge(str(parent_name), right_name, label='No')\n",
    "\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_ifreal(series: pd.Series) -> bool:\n",
    "    \"\"\"\n",
    "    Function to check if the given series has real or discrete values\n",
    "    \"\"\"\n",
    "    return pd.api.types.is_numeric_dtype(series)\n",
    "\n",
    "def entropy(series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the entropy of a given series\n",
    "    \"\"\"\n",
    "    value_counts = series.value_counts()\n",
    "    probabilities = value_counts / len(series)\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "\n",
    "def gini_index(series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the Gini index of a given series\n",
    "    \"\"\"\n",
    "    value_counts = series.value_counts()\n",
    "    probabilities = value_counts / len(series)\n",
    "    gini_index_value = 1 - np.sum(np.square(probabilities))\n",
    "    return gini_index_value\n",
    "\n",
    "def information_gain(y: pd.Series, attr: pd.Series, criterion) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the information gain\n",
    "    \"\"\"\n",
    "    if criterion == 'information_gain':\n",
    "        entropy_before = entropy(y)\n",
    "        entropy_after = y.groupby(attr).apply(lambda group: len(group) / len(y) * entropy(group)).sum()\n",
    "        return entropy_before - entropy_after\n",
    "    elif criterion == 'gini':\n",
    "        return y.groupby(attr).apply(lambda group: len(group) / len(y) * gini_index(group)).sum()\n",
    "    elif criterion == 'mse':\n",
    "        mse_before = y.var()\n",
    "        mse_after = y.groupby(attr).apply(lambda group: len(group) / len(y) * group.var()).sum()\n",
    "        return mse_before - mse_after\n",
    "    else:\n",
    "        raise ValueError(\"Invalid criterion\")\n",
    "\n",
    "def opt_split_attribute(X: pd.DataFrame, y: pd.Series, criterion, features: pd.Series):\n",
    "    \"\"\"\n",
    "    Function to find the optimal attribute to split upon.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "\n",
    "    for feature in features:\n",
    "        scores[feature] = information_gain(y, X[feature], criterion)\n",
    "    \n",
    "    if criterion in ['information_gain', 'mse']:\n",
    "        key = max(scores, key=scores.get)\n",
    "        return key, scores[key]\n",
    "    elif criterion == 'gini':\n",
    "        key = min(scores, key=scores.get)\n",
    "        return key, scores[key]\n",
    "\n",
    "def find_optimal_split_value(X: pd.DataFrame, y: pd.Series, attribute):\n",
    "    \"\"\"\n",
    "    Function to find the optimal split value for a given attribute.\n",
    "    \"\"\"\n",
    "    X_sorted = X.sort_values(by=attribute)\n",
    "    unique_values = (X_sorted[attribute] + X_sorted[attribute].shift()) / 2\n",
    "    unique_values = unique_values.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    y = y if check_ifreal(y) else y.cat.codes\n",
    "    best_mse = float('inf')\n",
    "    optimal_value = None\n",
    "\n",
    "    for value in unique_values:\n",
    "        mse = np.sum((y[X[attribute] <= value] - y[X[attribute] <= value].mean())**2) + \\\n",
    "              np.sum((y[X[attribute] > value] - y[X[attribute] > value].mean())**2)\n",
    "            \n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            optimal_value = value\n",
    "\n",
    "    return optimal_value\n",
    "\n",
    "def split_data(X: pd.DataFrame, y: pd.Series, attribute, value=None):\n",
    "    \"\"\"\n",
    "    Function to split the data according to an attribute.\n",
    "    \"\"\"\n",
    "    if not check_ifreal(X[attribute]):\n",
    "        unique_values = X[attribute].unique()\n",
    "        return [(X[X[attribute] == val], y[X[attribute] == val]) for val in unique_values], unique_values\n",
    "    else:\n",
    "        mask = (X[attribute] <= value)\n",
    "        return [(X[mask], y[mask]), (X[~mask], y[~mask])], value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def accuracy(y_hat: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the accuracy\n",
    "    \"\"\"\n",
    "    assert y_hat.size == y.size, \"Input sizes mismatch\"\n",
    "    correct_predictions = (y_hat == y).sum()\n",
    "    total_predictions = y.size\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "def precision(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the precision\n",
    "    \"\"\"\n",
    "    assert y_hat.size == y.size, \"Input sizes mismatch\"\n",
    "    \n",
    "    true_positive = ((y_hat == cls) & (y == cls)).sum()\n",
    "    false_positive = ((y_hat == cls) & (y != cls)).sum()\n",
    "    \n",
    "    if true_positive + false_positive == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return true_positive / (true_positive + false_positive)\n",
    "\n",
    "def recall(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the recall\n",
    "    \"\"\"\n",
    "    assert y_hat.size == y.size, \"Input sizes mismatch\"\n",
    "    \n",
    "    true_positive = ((y_hat == cls) & (y == cls)).sum()\n",
    "    false_negative = ((y_hat != cls) & (y == cls)).sum()\n",
    "    \n",
    "    if true_positive + false_negative == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return true_positive / (true_positive + false_negative)\n",
    "\n",
    "def rmse(y_hat: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the root-mean-squared-error (rmse)\n",
    "    \"\"\"\n",
    "    assert y_hat.size == y.size, \"Input sizes mismatch\"\n",
    "    return np.sqrt(((y_hat - y) ** 2).mean())\n",
    "\n",
    "def mae(y_hat: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the mean-absolute-error (mae)\n",
    "    \"\"\"\n",
    "    assert y_hat.size == y.size, \"Input sizes mismatch\"\n",
    "    return np.abs(y_hat - y).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def accuracy(y_hat: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the accuracy\n",
    "    \"\"\"\n",
    "    assert y_hat.size == y.size, \"Input sizes mismatch\"\n",
    "    correct_predictions = (y_hat == y).sum()\n",
    "    total_predictions = y.size\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "def precision(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the precision\n",
    "    \"\"\"\n",
    "    assert y_hat.size == y.size, \"Input sizes mismatch\"\n",
    "    \n",
    "    true_positive = ((y_hat == cls) & (y == cls)).sum()\n",
    "    false_positive = ((y_hat == cls) & (y != cls)).sum()\n",
    "    \n",
    "    if true_positive + false_positive == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return true_positive / (true_positive + false_positive)\n",
    "\n",
    "def recall(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the recall\n",
    "    \"\"\"\n",
    "    assert y_hat.size == y.size, \"Input sizes mismatch\"\n",
    "    \n",
    "    true_positive = ((y_hat == cls) & (y == cls)).sum()\n",
    "    false_negative = ((y_hat != cls) & (y == cls)).sum()\n",
    "    \n",
    "    if true_positive + false_negative == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return true_positive / (true_positive + false_negative)\n",
    "\n",
    "def rmse(y_hat: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the root-mean-squared-error (rmse)\n",
    "    \"\"\"\n",
    "    assert y_hat.size == y.size, \"Input sizes mismatch\"\n",
    "    return np.sqrt(((y_hat - y) ** 2).mean())\n",
    "\n",
    "def mae(y_hat: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the mean-absolute-error (mae)\n",
    "    \"\"\"\n",
    "    assert y_hat.size == y.size, \"Input sizes mismatch\"\n",
    "    return np.abs(y_hat - y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(None, None, -inf)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     28\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfind_optimal_split_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_best_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     30\u001b[0m tree\u001b[38;5;241m.\u001b[39mplot()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCriteria :\u001b[39m\u001b[38;5;124m\"\u001b[39m, criteria)\n",
      "Cell \u001b[1;32mIn[2], line 65\u001b[0m, in \u001b[0;36mfind_optimal_split_value\u001b[1;34m(X, y, attribute)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_optimal_split_value\u001b[39m(X: pd\u001b[38;5;241m.\u001b[39mDataFrame, y: pd\u001b[38;5;241m.\u001b[39mSeries, attribute):\n\u001b[0;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    Function to find the optimal split value for a given attribute.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     X_sorted \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     unique_values \u001b[38;5;241m=\u001b[39m (X_sorted[attribute] \u001b[38;5;241m+\u001b[39m X_sorted[attribute]\u001b[38;5;241m.\u001b[39mshift()) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     67\u001b[0m     unique_values \u001b[38;5;241m=\u001b[39m unique_values\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:6758\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6754\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(by):\n\u001b[0;32m   6755\u001b[0m     \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[0;32m   6757\u001b[0m     by \u001b[38;5;241m=\u001b[39m by[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 6758\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label_or_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6760\u001b[0m     \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[0;32m   6761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   6762\u001b[0m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[0;32m   6763\u001b[0m         \u001b[38;5;66;03m# \"Series\", variable has type \"ndarray\")\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:1778\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1776\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mget_level_values(key)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: (None, None, -inf)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The current code given is for the Assignment 1.\n",
    "You will be expected to use this to make trees for:\n",
    "> discrete input, discrete output\n",
    "> real input, real output\n",
    "> real input, discrete output\n",
    "> discrete input, real output\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "np.random.seed(42)\n",
    "# Test case 1\n",
    "# Real Input and Real Output\n",
    "\n",
    "N = 30\n",
    "P = 5\n",
    "X = pd.DataFrame(np.random.randn(N, P))\n",
    "y = pd.Series(np.random.randn(N))\n",
    "\n",
    "\n",
    "for criteria in [\"information_gain\", \"gini_index\"]:\n",
    "    tree = DecisionTree(criterion=criteria,max_depth=9)  # Split based on Inf. Gain\n",
    "    tree.fit(X, y)\n",
    "    y_hat = tree.predict(X)\n",
    "    print(find_optimal_split_value(X, y, tree.find_best_split(X, y)))\n",
    "    tree.plot()\n",
    "    print(\"Criteria :\", criteria)\n",
    "    print(\"RMSE: \", rmse(y_hat, y))\n",
    "    print(\"MAE: \", mae(y_hat, y))\n",
    "\n",
    "# Test case 2\n",
    "# Real Input and Discrete Output\n",
    "\n",
    "N = 30\n",
    "P = 5\n",
    "X = pd.DataFrame(np.random.randn(N, P))\n",
    "y = pd.Series(np.random.randint(P, size=N), dtype=\"category\")\n",
    "\n",
    "for criteria in [\"information_gain\", \"gini_index\"]:\n",
    "    tree = DecisionTree(criterion=criteria,max_depth=9)  # Split based on Inf. Gain\n",
    "    tree.fit(X, y)\n",
    "    y_hat = tree.predict(X)\n",
    "    tree.plot()\n",
    "    print(\"Criteria :\", criteria)\n",
    "    print(\"Accuracy: \", accuracy(y_hat, y))\n",
    "    for cls in y.unique():\n",
    "        print(\"Precision: \", precision(y_hat, y, cls))\n",
    "        print(\"Recall: \", recall(y_hat, y, cls))\n",
    "\n",
    "\n",
    "# Test case 3\n",
    "# Discrete Input and Discrete Output\n",
    "\n",
    "N = 30\n",
    "P = 5\n",
    "X = pd.DataFrame({i: pd.Series(np.random.randint(P, size=N), dtype=\"category\") for i in range(5)})\n",
    "y = pd.Series(np.random.randint(P, size=N), dtype=\"category\")\n",
    "\n",
    "for criteria in [\"information_gain\", \"gini_index\"]:\n",
    "    tree = DecisionTree(criterion=criteria,max_depth=9)  # Split based on Inf. Gain\n",
    "    tree.fit(X, y)\n",
    "    y_hat = tree.predict(X)\n",
    "    tree.plot()\n",
    "    print(\"Criteria :\", criteria)\n",
    "    print(\"Accuracy: \", accuracy(y_hat, y))\n",
    "    for cls in y.unique():\n",
    "        print(\"Precision: \", precision(y_hat, y, cls))\n",
    "        print(\"Recall: \", recall(y_hat, y, cls))\n",
    "\n",
    "# Test case 4\n",
    "# Discrete Input and Real Output\n",
    "\n",
    "N = 30\n",
    "P = 5\n",
    "X = pd.DataFrame({i: pd.Series(np.random.randint(P, size=N), dtype=\"category\") for i in range(5)})\n",
    "y = pd.Series(np.random.randn(N))\n",
    "\n",
    "for criteria in [\"information_gain\", \"gini_index\"]:\n",
    "    tree = DecisionTree(criterion=criteria,max_depth=9)  # Split based on Inf. Gain\n",
    "    tree.fit(X, y)\n",
    "    y_hat = tree.predict(X)\n",
    "    tree.plot()\n",
    "    \n",
    "    print(\"Criteria :\", criteria)\n",
    "    print(\"RMSE: \", rmse(y_hat, y))\n",
    "    print(\"MAE: \", mae(y_hat, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
